{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyse des erreurs de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if arima_model is not None and lstm_model is not None:\n",
    "    # Calcul des erreurs de prédiction\n",
    "    arima_errors = y_true[:len(y_pred_arima)] - y_pred_arima\n",
    "    lstm_errors = y_test - y_pred_lstm\n",
    "    \n",
    "    # Distribution des erreurs\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(arima_errors, kde=True, color='red')\n",
    "    plt.title('Distribution des erreurs ARIMA')\n",
    "    plt.xlabel('Erreur')\n",
    "    plt.ylabel('Fréquence')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(lstm_errors, kde=True, color='green')\n",
    "    plt.title('Distribution des erreurs LSTM')\n",
    "    plt.xlabel('Erreur')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Erreurs au fil du temps\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(df_test.index[:len(arima_errors)], arima_errors, label='Erreurs ARIMA', color='red', alpha=0.7)\n",
    "    plt.plot(test_indices, lstm_errors, label='Erreurs LSTM', color='green', alpha=0.7)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.title('Erreurs de prédiction au fil du temps', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Erreur (Valeur réelle - Prédiction)', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyse des performances par saison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if arima_model is not None and lstm_model is not None:\n",
    "    # Ajout d'une colonne pour la saison\n",
    "    def get_season(month):\n",
    "        if month in [12, 1, 2]:\n",
    "            return 'Hiver'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 'Printemps'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 'Été'\n",
    "        else:\n",
    "            return 'Automne'\n",
    "    \n",
    "    # Création d'un DataFrame avec les erreurs\n",
    "    error_df = pd.DataFrame({\n",
    "        'Date': df_test.index[:len(arima_errors)],\n",
    "        'ARIMA_Error': np.abs(arima_errors),\n",
    "        'LSTM_Error': np.abs(lstm_errors[:len(arima_errors)]) if len(lstm_errors) >= len(arima_errors) else np.pad(np.abs(lstm_errors), (0, len(arima_errors) - len(lstm_errors)), 'constant')\n",
    "    })\n",
    "    \n",
    "    error_df['Month'] = error_df['Date'].dt.month\n",
    "    error_df['Season'] = error_df['Month'].apply(get_season)\n",
    "    \n",
    "    # Analyse des erreurs par saison\n",
    "    season_errors = error_df.groupby('Season').agg({\n",
    "        'ARIMA_Error': 'mean',\n",
    "        'LSTM_Error': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    season_order = ['Hiver', 'Printemps', 'Été', 'Automne']\n",
    "    season_errors = season_errors.set_index('Season').loc[season_order].reset_index()\n",
    "    \n",
    "    # Visualisation des erreurs par saison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(season_errors))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, season_errors['ARIMA_Error'], width, label='ARIMA', color='red', alpha=0.7)\n",
    "    plt.bar(x + width/2, season_errors['LSTM_Error'], width, label='LSTM', color='green', alpha=0.7)\n",
    "    \n",
    "    plt.title('Erreur moyenne absolue par saison', fontsize=16)\n",
    "    plt.xlabel('Saison', fontsize=12)\n",
    "    plt.ylabel('MAE', fontsize=12)\n",
    "    plt.xticks(x, season_errors['Season'])\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "À partir de l'évaluation des modèles, nous pouvons tirer les conclusions suivantes :\n",
    "\n",
    "1. **Performances globales** : [à compléter après analyse]\n",
    "2. **Forces et faiblesses du modèle ARIMA** : [à compléter après analyse]\n",
    "3. **Forces et faiblesses du modèle LSTM** : [à compléter après analyse]\n",
    "4. **Recommandations pour l'utilisation en production** : [à compléter après analyse]\n",
    "5. **Pistes d'amélioration** : [à compléter après analyse]\n",
    "\n",
    "Ces conclusions nous permettront de choisir le modèle le plus adapté pour le déploiement dans notre application de prévision d'affluence hospitalière."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
},
   "source": [
    "# Évaluation des modèles de prévision d'affluence hospitalière\n",
    "\n",
    "Ce notebook compare les performances des différents modèles (ARIMA et LSTM) sur les données d'affluence hospitalière."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Ajout du répertoire parent au path pour importer les modules du projet\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import des fonctions du projet\n",
    "from src.data.data_loader import load_hospital_data\n",
    "from src.data.make_dataset import preprocess_data\n",
    "from src.features.build_features import extract_temporal_features\n",
    "from src.models.arima_model import ARIMAModel\n",
    "from src.models.lstm_model import LSTMModel\n",
    "from src.models.train_model import evaluate_model\n",
    "from src.visualization.visualize import plot_predictions\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set(font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et prétraitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Chargement des données\n",
    "data_path = '../data/raw/hospital_data.csv'\n",
    "df = load_hospital_data(data_path)\n",
    "\n",
    "# Prétraitement des données\n",
    "df_processed = preprocess_data(df)\n",
    "\n",
    "# Extraction des features temporelles\n",
    "df_features = extract_temporal_features(df_processed)\n",
    "\n",
    "# Affichage des données prétraitées\n",
    "print(f\"Dimensions des données prétraitées: {df_features.shape}\")\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Division des données en ensembles d'entraînement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Division des données\n",
    "train_size = int(len(df_features) * 0.8)\n",
    "df_train = df_features[:train_size]\n",
    "df_test = df_features[train_size:]\n",
    "\n",
    "print(f\"Taille de l'ensemble d'entraînement: {df_train.shape}\")\n",
    "print(f\"Taille de l'ensemble de test: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement des modèles entraînés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Chargement du modèle ARIMA\n",
    "try:\n",
    "    arima_model_path = '../models/arima_model.pkl'\n",
    "    with open(arima_model_path, 'rb') as f:\n",
    "        arima_model = pickle.load(f)\n",
    "    print(\"Modèle ARIMA chargé avec succès.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Modèle ARIMA non trouvé. Veuillez d'abord entraîner le modèle.\")\n",
    "    arima_model = None\n",
    "\n",
    "# Chargement du modèle LSTM\n",
    "try:\n",
    "    lstm_model_path = '../models/lstm_model.h5'\n",
    "    lstm_scaler_path = '../models/lstm_scaler.pkl'\n",
    "    \n",
    "    lstm_model = LSTMModel()\n",
    "    lstm_model.load_model(lstm_model_path, lstm_scaler_path)\n",
    "    print(\"Modèle LSTM chargé avec succès.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Modèle LSTM non trouvé. Veuillez d'abord entraîner le modèle.\")\n",
    "    lstm_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Évaluation du modèle ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if arima_model is not None:\n",
    "    # Faire des prédictions avec le modèle ARIMA\n",
    "    y_true = df_test['patient_count'].values\n",
    "    y_pred_arima = arima_model.predict(len(y_true))\n",
    "    \n",
    "    # Calcul des métriques d'évaluation\n",
    "    mse_arima = mean_squared_error(y_true, y_pred_arima)\n",
    "    rmse_arima = np.sqrt(mse_arima)\n",
    "    mae_arima = mean_absolute_error(y_true, y_pred_arima)\n",
    "    r2_arima = r2_score(y_true, y_pred_arima)\n",
    "    \n",
    "    print(\"Métriques d'évaluation du modèle ARIMA:\")\n",
    "    print(f\"MSE: {mse_arima:.4f}\")\n",
    "    print(f\"RMSE: {rmse_arima:.4f}\")\n",
    "    print(f\"MAE: {mae_arima:.4f}\")\n",
    "    print(f\"R²: {r2_arima:.4f}\")\n",
    "    \n",
    "    # Visualisation des prédictions\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.plot(df_test.index, y_true, label='Valeurs réelles', color='blue')\n",
    "    plt.plot(df_test.index, y_pred_arima, label='Prédictions ARIMA', color='red', linestyle='--')\n",
    "    plt.title('Comparaison des prédictions ARIMA avec les valeurs réelles', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Nombre de patients', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Évaluation du modèle LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if lstm_model is not None:\n",
    "    # Préparation des données pour le modèle LSTM\n",
    "    # Supposons que X_test et y_test sont déjà préparés correctement pour le modèle LSTM\n",
    "    sequence_length = 10  # À ajuster en fonction de votre modèle LSTM\n",
    "    X_test, y_test = lstm_model.prepare_data(df_test, 'patient_count', sequence_length)\n",
    "    \n",
    "    # Faire des prédictions avec le modèle LSTM\n",
    "    y_pred_lstm = lstm_model.predict(X_test)\n",
    "    \n",
    "    # Calcul des métriques d'évaluation\n",
    "    mse_lstm = mean_squared_error(y_test, y_pred_lstm)\n",
    "    rmse_lstm = np.sqrt(mse_lstm)\n",
    "    mae_lstm = mean_absolute_error(y_test, y_pred_lstm)\n",
    "    r2_lstm = r2_score(y_test, y_pred_lstm)\n",
    "    \n",
    "    print(\"\\nMétriques d'évaluation du modèle LSTM:\")\n",
    "    print(f\"MSE: {mse_lstm:.4f}\")\n",
    "    print(f\"RMSE: {rmse_lstm:.4f}\")\n",
    "    print(f\"MAE: {mae_lstm:.4f}\")\n",
    "    print(f\"R²: {r2_lstm:.4f}\")\n",
    "    \n",
    "    # Visualisation des prédictions\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    # Ajustement des indices pour l'affichage\n",
    "    test_indices = df_test.index[sequence_length:sequence_length+len(y_test)]\n",
    "    plt.plot(test_indices, y_test, label='Valeurs réelles', color='blue')\n",
    "    plt.plot(test_indices, y_pred_lstm, label='Prédictions LSTM', color='green', linestyle='--')\n",
    "    plt.title('Comparaison des prédictions LSTM avec les valeurs réelles', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Nombre de patients', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparaison des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if arima_model is not None and lstm_model is not None:\n",
    "    # Création d'un DataFrame pour la comparaison\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Modèle': ['ARIMA', 'LSTM'],\n",
    "        'MSE': [mse_arima, mse_lstm],\n",
    "        'RMSE': [rmse_arima, rmse_lstm],\n",
    "        'MAE': [mae_arima, mae_lstm],\n",
    "        'R²': [r2_arima, r2_lstm]\n",
    "    })\n",
    "    \n",
    "    print(\"Comparaison des performances des modèles:\")\n",
    "    display(metrics_df)\n",
    "    \n",
    "    # Visualisation comparative\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    metrics = ['MSE', 'RMSE', 'MAE']\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    rects1 = ax.bar(x - width/2, [mse_arima, rmse_arima, mae_arima], width, label='ARIMA')\n",
    "    rects2 = ax.